# Workshop 1 Machine Learning II
workshop 1 machine learning.  This is my firt repository in a Organization GitHub.

# PREGUNTA 8:
## Para hacer PCA más robusto, puedes considerar algunas de las siguientes estrategias:

Estandarizar las variables: Antes de realizar el PCA, es recomendable estandarizar las variables para asegurarse de que todas estén en la misma escala. Esto ayuda a que las variables que tienen valores más altos no dominen el análisis y, por lo tanto, no afecten los resultados de PCA.

Tratar los valores atípicos: Los valores atípicos pueden afectar significativamente los resultados de PCA. Por lo tanto, es importante detectar y tratar los valores atípicos antes de realizar el análisis. Una forma de hacerlo es utilizando técnicas como la eliminación de valores atípicos o la sustitución de valores atípicos con valores más cercanos al promedio.

Usar diferentes métodos de rotación: Los métodos de rotación pueden afectar los resultados de PCA. Por lo tanto, es recomendable probar diferentes métodos de rotación, como la rotación Varimax o la rotación Oblimin, para ver cuál funciona mejor para los datos en cuestión.

# PREGUNTA 9:
UMAP (Uniform Manifold Approximation and Projection) es una técnica de reducción de la dimensionalidad no lineal utilizada en aprendizaje automático y minería de datos para visualizar y explorar grandes conjuntos de datos. UMAP se basa en el principio matemático de la topología algebraica y utiliza técnicas de aprendizaje automático para encontrar una representación en una dimensión inferior de los datos originales

# PREGUNTA 10: 
LDA (Linear Discriminant Analysis) es una técnica de reducción de la dimensionalidad utilizada en aprendizaje automático y minería de datos para identificar las características que mejor diferencian entre dos o más clases de objetos.
El principio matemático de LDA se basa en el cálculo de una función de discriminación lineal que maximiza la relación entre la varianza entre clases y la varianza dentro de las clases. 